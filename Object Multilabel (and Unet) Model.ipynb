{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d8bd8b",
   "metadata": {},
   "source": [
    "# Object Multilabel (and Unet) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.nn.utils\n",
    "from torch.autograd import Function\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eada66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, factor, ngf=64):\n",
    "        super(Down, self).__init__()\n",
    "        \n",
    "        self.downRelu = nn.LeakyReLU(0.2, True)\n",
    "        self.downSample = nn.Conv2d(ngf * factor, ngf * 2 * factor, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downNorm = norm_layer(ngf * 2 * factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_down = self.downNorm(self.downSample(self.downRelu(x)))\n",
    "        \n",
    "        return x_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, factor, ngf=64):\n",
    "        super(Down, self).__init__()\n",
    "        \n",
    "        self.downRelu = nn.LeakyReLU(0.2, True)\n",
    "        self.downSample = nn.Conv2d(ngf * factor, ngf * 2 * factor, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downNorm = norm_layer(ngf * 2 * factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_down = self.downNorm(self.downSample(self.downRelu(x)))\n",
    "        \n",
    "        return x_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4655993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with skip connection and pixel connection and smoothed\n",
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        use_bias = True\n",
    "        # construct unet structure\n",
    "        self.downsample_0 = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "\n",
    "        self.downRelu_1 = nn.LeakyReLU(0.2, True)\n",
    "        self.downSample_1 = nn.Conv2d(ngf, ngf * 2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downNorm_1 = norm_layer(ngf * 2)\n",
    "\n",
    "        self.downRelu_2 = nn.LeakyReLU(0.2, True)\n",
    "        self.downSample_2 = nn.Conv2d(ngf * 2, ngf * 4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downNorm_2 = norm_layer(ngf * 4)\n",
    "\n",
    "        self.downRelu_3 = nn.LeakyReLU(0.2, True)\n",
    "        self.downSample_3 = nn.Conv2d(ngf * 4, ngf * 8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downNorm_3 = norm_layer(ngf * 8)\n",
    "\n",
    "        self.innerLeakyRelu = nn.LeakyReLU(0.2, True)\n",
    "        self.innerDownSample = nn.Conv2d(ngf * 8, ngf * 8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "\n",
    "        self.innerRelu = nn.ReLU(True)\n",
    "        innerUpSample = []\n",
    "        innerUpSample.append(nn.Upsample(scale_factor = 2, mode='bilinear'))\n",
    "        innerUpSample.append(nn.ReflectionPad2d((2, 1, 2, 1)))\n",
    "        innerUpSample.append(nn.Conv2d(ngf * 8, ngf * 8, kernel_size=4, stride=1, padding=0, bias=use_bias))\n",
    "        self.innerUpSample = nn.Sequential(*innerUpSample)\n",
    "\n",
    "        self.innerNorm = norm_layer(ngf * 8)\n",
    "\n",
    "        self.upRelu_3 = nn.ReLU(True)\n",
    "        upSample_3 = []\n",
    "        upSample_3.append(nn.Upsample(scale_factor = 2, mode='bilinear'))\n",
    "        upSample_3.append(nn.ReflectionPad2d((2, 1, 2, 1)))\n",
    "        upSample_3.append(nn.Conv2d(ngf * 16, ngf * 4, kernel_size=4, stride=1, padding=0, bias=use_bias))\n",
    "        self.upSample_3 = nn.Sequential(*upSample_3)\n",
    "        self.upNorm_3 = norm_layer(ngf * 4)\n",
    "\n",
    "        self.upRelu_2 = nn.ReLU(True)\n",
    "        upSample_2 = []\n",
    "        upSample_2.append(nn.Upsample(scale_factor = 2, mode='bilinear'))\n",
    "        upSample_2.append(nn.ReflectionPad2d((2, 1, 2, 1)))\n",
    "        upSample_2.append(nn.Conv2d(ngf * 8, ngf * 2, kernel_size=4, stride=1, padding=0, bias=use_bias))\n",
    "        self.upSample_2 = nn.Sequential(*upSample_2)\n",
    "        self.upNorm_2 = norm_layer(ngf * 2)\n",
    "\n",
    "        self.upRelu_1 = nn.ReLU(True)\n",
    "        upSample_1 = []\n",
    "        upSample_1.append(nn.Upsample(scale_factor = 2, mode='bilinear'))\n",
    "        upSample_1.append(nn.ReflectionPad2d((2, 1, 2, 1)))\n",
    "        upSample_1.append(nn.Conv2d(ngf * 4, ngf, kernel_size=4, stride=1, padding=0, bias=use_bias))\n",
    "        self.upSample_1 = nn.Sequential(*upSample_1)\n",
    "        self.upNorm_1 = norm_layer(ngf)\n",
    "\n",
    "        self.upRelu_0 = nn.ReLU(True)\n",
    "        upSample_0 = []\n",
    "        upSample_0.append(nn.Upsample(scale_factor = 2, mode='bilinear'))\n",
    "        upSample_0.append(nn.ReflectionPad2d((2, 1, 2, 1)))\n",
    "        upSample_0.append(nn.Conv2d(ngf * 2, 1, kernel_size=4, stride=1, padding=0, bias=use_bias))\n",
    "        self.upSample_0 = nn.Sequential(*upSample_0)\n",
    "\n",
    "        ## initialize bias\n",
    "        nn.init.normal_(self.upSample_0[-1].bias, mean=3, std=1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # assume input image size = 224\n",
    "        x_down_0 = self.downsample_0(input) # (ngf, 112, 112)\n",
    "\n",
    "        x_down_1 = self.downNorm_1(self.downSample_1(self.downRelu_1(x_down_0))) # (ngf*2, 56, 56)\n",
    "        x_down_2 = self.downNorm_2(self.downSample_2(self.downRelu_2(x_down_1))) # (ngf*4, 28, 28)\n",
    "        x_down_3 = self.downNorm_3(self.downSample_3(self.downRelu_3(x_down_2))) # (ngf*8, 14, 14)\n",
    "\n",
    "        latent = self.innerDownSample(self.innerLeakyRelu(x_down_3)) # (ngf*8, 7, 7)\n",
    "\n",
    "        x = self.innerNorm(self.innerUpSample(self.innerRelu(latent))) # (ngf*8, 14, 14)\n",
    "\n",
    "        x_up_3 = self.upNorm_3(self.upSample_3(self.upRelu_3(torch.cat([x, x_down_3], 1)))) # (ngf*4, 28, 28)\n",
    "        x_up_2 = self.upNorm_2(self.upSample_2(self.upRelu_2(torch.cat([x_up_3, x_down_2], 1)))) # (ngf*2, 56, 56)\n",
    "        x_up_1 = self.upNorm_1(self.upSample_1(self.upRelu_1(torch.cat([x_up_2, x_down_1], 1)))) # (ngf, 112, 112)\n",
    "\n",
    "        encoded_image = self.activation(self.upSample_0(self.upRelu_0(torch.cat([x_up_1, x_down_0], 1)))) # (3, 224, 224)\n",
    "\n",
    "        return torch.mul(input, encoded_image), latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectMultiLabelAdv(nn.Module):\n",
    "\n",
    "    def __init__(self, args, num_object, hid_size, dropout, adv_lambda):\n",
    "\n",
    "        super(ObjectMultiLabelAdv, self).__init__()\n",
    "        print(\"Build a ObjectMultiLabelAdv Model[{}]\".format(args.layer))\n",
    "        self.num_object = num_object\n",
    "        self.args = args\n",
    "        self.base_network = models.resnet50(pretrained = True)\n",
    "        self.adv_lambda = adv_lambda\n",
    "        print('Load weights from Resnet18/50 done')\n",
    "\n",
    "        norm_layer = 'batch'\n",
    "        use_dropout = False\n",
    "        norm_layer = get_norm_layer(norm_type=norm_layer)\n",
    "        self.autoencoder = UnetGenerator(3, 3, 5, 64, \\\n",
    "            norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "\n",
    "        output_size = self.num_object\n",
    "        self.finalLayer = nn.Linear(self.base_network.fc.in_features, output_size)\n",
    "\n",
    "        if not args.autoencoder_finetune:\n",
    "            for param in self.autoencoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if not args.finetune:\n",
    "            for param in self.base_network.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.finalLayer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        assert  args.layer == 'generated_image'\n",
    "        self.adv_component = GenderClassification(args)\n",
    "        pretrained_gender_classifier_path = './model_best_object_balanced.pth.tar'\n",
    "        gender_clssifier_checkpoint = torch.load(pretrained_gender_classifier_path)\n",
    "        self.adv_component.load_state_dict(gender_clssifier_checkpoint['state_dict'])\n",
    "        print(\"Loaded pretrained gender classifier from {}\".format(pretrained_gender_classifier_path))\n",
    "\n",
    "        if not args.finetune:\n",
    "            for param in self.adv_component.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        autoencoded_image, latent = self.autoencoder(image)\n",
    "\n",
    "        x = self.base_network.conv1(autoencoded_image)\n",
    "        x = self.base_network.bn1(x)\n",
    "        x = self.base_network.relu(x)\n",
    "        conv1_feature = self.base_network.maxpool(x)\n",
    "\n",
    "        layer1_feature = self.base_network.layer1(conv1_feature)\n",
    "        layer2_feature = self.base_network.layer2(layer1_feature)\n",
    "        layer3_feature = self.base_network.layer3(layer2_feature)\n",
    "        layer4_feature = self.base_network.layer4(layer3_feature)\n",
    "\n",
    "        final_feature = self.base_network.avgpool(layer4_feature)\n",
    "        final_feature = final_feature.view(final_feature.size(0), -1)\n",
    "\n",
    "        preds = self.finalLayer(final_feature)\n",
    "\n",
    "        adv_feature = ReverseLayerF.apply(autoencoded_image, self.adv_lambda)\n",
    "        adv_preds = self.adv_component(adv_feature)\n",
    "        return preds, adv_preds, autoencoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dddac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
